{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VagnLcLKVFjS",
        "outputId": "923b483e-e09a-4ee5-e275-7ae768dea2e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import multiprocessing"
      ],
      "metadata": {
        "id": "cg1mtj_4VCVK"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the preprocessed data from products_train_processed.csv\n",
        "df_processed = pd.read_csv('/content/drive/MyDrive/products_train_processed.csv')\n",
        "\n",
        "# Filter the data for the UK locale\n",
        "df_uk = df_processed[df_processed['locale'] == 'UK']\n",
        "\n",
        "# Combine 'brand' and 'title' columns to create a new 'text' column\n",
        "df_uk['text'] = df_uk['brand'] + ' ' + df_uk['title']\n",
        "\n",
        "# Remove missing values from the 'text' column\n",
        "df_uk = df_uk.dropna(subset=['text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmbac48ubnev",
        "outputId": "968e157d-cdab-4094-8664-780b8242cc49"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-393a00d61e73>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df_uk['text'] = df_uk['brand'] + ' ' + df_uk['title']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_uk['text'].head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KadFxyjWhyyy",
        "outputId": "83823846-b585-430d-9d18-f7e685e39c69"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "913336    sochow sochow sherpa fleec throw blanket doubl...\n",
            "913337    hippowareh hippowareh personali photo print mo...\n",
            "913338    clarkson potter 500 easi recip everi machin st...\n",
            "913339    tyhjoy tyhjoy mini bag sealer handheld vacuum ...\n",
            "913340    lucosobi lucosobi steer wheel lock car antithe...\n",
            "913341                               88 film tentacl bluray\n",
            "913342    bochion knife sharpen 4 1 finger protect knife...\n",
            "913343    spigen spigen rug armor design iphon 11 case 2...\n",
            "913344    criacr criacr motion sensor light bar 10 led 6...\n",
            "913345    coolzon coolzon bento box lunch box adult kids...\n",
            "Name: text, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the text\n",
        "tokenized_text = [text.split() for text in df_uk['text']]\n"
      ],
      "metadata": {
        "id": "M31g6zt9br2i"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Word2Vec model\n",
        "cores = multiprocessing.cpu_count()  # Number of CPU cores\n",
        "model = Word2Vec(tokenized_text,\n",
        "                 vector_size=100,  # Size of the word embeddings\n",
        "                 window=5,  # Context window size\n",
        "                 min_count=5,  # Minimum word frequency threshold\n",
        "                 workers=cores,  # Number of workers for parallel processing\n",
        "                 sg=0)  # Training algorithm: 0 for CBOW, 1 for skip-gram\n",
        "\n",
        "# Optional: Train a phrase model to capture common phrases (e.g., \"green_haven\")\n",
        "phrases = Phrases(tokenized_text, min_count=5, threshold=10)\n",
        "bigram = Phraser(phrases)\n",
        "tokenized_text_phrases = [bigram[doc] for doc in tokenized_text]\n",
        "\n"
      ],
      "metadata": {
        "id": "kYzbN1Dkbwhm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the Word2Vec model on the tokenized text with phrases\n",
        "model_phrases = Word2Vec(tokenized_text_phrases, vector_size=100, window=5, min_count=5, workers=cores, sg=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "xVaduMQ2b5_0"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "model.save(\"word2vec_uk.model\")\n",
        "model_phrases.save(\"word2vec_phrases_uk.model\")"
      ],
      "metadata": {
        "id": "FL2GaX99b8GP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Access a sample item from the dataset\n",
        "sample_item = df_uk.iloc[1]  # Assuming the first item in the filtered dataframe\n",
        "sample_text = sample_item['text']\n",
        "product_id = sample_item['id']\n",
        "\n",
        "# Print the word embeddings and corresponding product ID for the sample item\n",
        "for word in sample_text.split():\n",
        "    if word in model.wv:\n",
        "        print(f\"Word: {word} | Embedding: {model.wv[word]} | Product ID: {product_id}\")"
      ],
      "metadata": {
        "id": "KExqcY_YeVdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load the saved word2vec models\n",
        "model = Word2Vec.load(\"word2vec_uk.model\")\n",
        "model_phrases = Word2Vec.load(\"word2vec_phrases_uk.model\")"
      ],
      "metadata": {
        "id": "MCOnUOMAhBnd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training data\n",
        "products_train = pd.read_csv(\"/content/drive/MyDrive/products_train_processed.csv\")\n",
        "sessions_train = pd.read_csv(\"/content/drive/MyDrive/sessions_train.csv\")\n",
        "\n",
        "# Filter the data for the UK locale\n",
        "uk_sessions_train = sessions_train[sessions_train['locale'] == 'UK']\n",
        "\n",
        "# Merge sessions_train with products_train to get the product information\n",
        "merged_data = pd.merge(uk_sessions_train, products_train, left_on='next_item', right_on='id', how='left')\n",
        "\n",
        "# Filter the data to include only the required columns\n",
        "filtered_data = merged_data[['prev_items', 'next_item']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "nYspv6hdvh0w",
        "outputId": "3e39da4f-84cb-419c-bb1b-80916b1c4841"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a20ad9975b36>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mproducts_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/products_train_processed.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msessions_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/sessions_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Filter the data for the UK locale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the prev_items column into individual product IDs\n",
        "filtered_data['prev_items'] = filtered_data['prev_items'].apply(lambda x: x.split())\n",
        "\n",
        "# Extract the input features and target labels\n",
        "input_features = filtered_data['prev_items']\n",
        "target_labels = filtered_data['next_item']"
      ],
      "metadata": {
        "id": "XZbFSofWwXpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Transformer\n",
        "\n",
        "# Define the transformer-based model\n",
        "def build_recommendation_model(vocab_size, embed_dim, num_heads, feed_forward_dim):\n",
        "    # Input layer\n",
        "    inputs = Input(shape=(None,))\n",
        "\n",
        "    # Embedding layer\n",
        "    embedding = Embedding(vocab_size, embed_dim)(inputs)\n",
        "\n",
        "    # Transformer layer\n",
        "    transformer = Transformer(num_heads=num_heads, d_model=embed_dim,\n",
        "                              ff_dim=feed_forward_dim, activation='relu')(embedding)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(vocab_size, activation='softmax')(transformer)\n",
        "\n",
        "    # Create the model\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "# Set the hyperparameters\n",
        "vocab_size = len(model.wv.vocab)  # Adjust based on your vocabulary size\n",
        "embed_dim = 300  # Adjust based on the dimensionality of your word embeddings\n",
        "num_heads = 4  # Adjust based on the desired number of attention heads\n",
        "feed_forward_dim = 512  # Adjust based on the desired size of the feed-forward layer\n",
        "\n",
        "# Build the recommendation model\n",
        "recommendation_model = build_recommendation_model(vocab_size, embed_dim, num_heads, feed_forward_dim)"
      ],
      "metadata": {
        "id": "2AvDtSN2wxXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function and optimizer\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# Compile the model\n",
        "recommendation_model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "# Convert the input features and target labels to tensors\n",
        "input_features = tf.ragged.constant(input_features.tolist())\n",
        "target_labels = tf.convert_to_tensor(target_labels)\n",
        "\n",
        "# Train the model for a few epochs\n",
        "num_epochs = 10\n",
        "recommendation_model.fit(input_features, target_labels, epochs=num_epochs)"
      ],
      "metadata": {
        "id": "sS1NWs7Bw3uK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "sessions_test = pd.read_csv(\"sessions_test_task1_phase1.csv\")\n",
        "uk_sessions_test = sessions_test[sessions_test['locale'] == 'UK']"
      ],
      "metadata": {
        "id": "8t68oysYyXXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the prev_items column into individual product IDs\n",
        "uk_sessions_test['prev_items'] = uk_sessions_test['prev_items'].apply(lambda x: x.split())\n",
        "\n",
        "# Convert the input features to a list\n",
        "input_features = uk_sessions_test['prev_items'].tolist()"
      ],
      "metadata": {
        "id": "9SdPKna_ycSv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the input features to tensors\n",
        "input_features = tf.ragged.constant(input_features)\n",
        "\n",
        "# Generate predictions\n",
        "predictions = recommendation_model.predict(input_features)\n",
        "\n",
        "# Get the top 100 recommended product IDs for each session\n",
        "recommended_product_ids = []\n",
        "for prediction in predictions:\n",
        "    top_product_ids = tf.argsort(prediction, direction='DESCENDING')[:100]\n",
        "    recommended_product_ids.append(top_product_ids)\n",
        "\n",
        "# Convert the recommended product IDs to a list\n",
        "recommended_product_ids = [ids.numpy().tolist() for ids in recommended_product_ids]"
      ],
      "metadata": {
        "id": "1iPUNGtiygdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame with the results\n",
        "results = pd.DataFrame({\n",
        "    'prev_items': uk_sessions_test['prev_items'],\n",
        "    'locale': uk_sessions_test['locale'],\n",
        "    'recommended_product_ids': recommended_product_ids\n",
        "})\n",
        "\n",
        "# Save the results to a CSV file\n",
        "results.to_csv(\"/content/drive/MyDrive/RESULTS.csv\", index=False)"
      ],
      "metadata": {
        "id": "ocj5kX-9zgTt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}