{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "08nHORvY8MqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNz4Ky7x7Yn_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.phrases import Phrases, Phraser\n",
        "import multiprocessing\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Embedding, Dense, Transformer\n",
        "\n",
        "# Function to train a Word2Vec model\n",
        "def train_word2vec_model(locale):\n",
        "    # Load the preprocessed data for the specified locale\n",
        "    df_processed = pd.read_csv('/content/drive/MyDrive/products_train_processed.csv')\n",
        "    df_locale = df_processed[df_processed['locale'] == locale]\n",
        "    df_locale['text'] = df_locale['brand'] + ' ' + df_locale['title']\n",
        "    df_locale = df_locale.dropna(subset=['text'])\n",
        "    tokenized_text = [text.split() for text in df_locale['text']]\n",
        "\n",
        "    # Train the Word2Vec model\n",
        "    cores = multiprocessing.cpu_count()\n",
        "    model = Word2Vec(tokenized_text, vector_size=100, window=5, min_count=5, workers=cores, sg=0)\n",
        "\n",
        "    # Optional: Train a phrase model to capture common phrases\n",
        "    phrases = Phrases(tokenized_text, min_count=5, threshold=10)\n",
        "    bigram = Phraser(phrases)\n",
        "    tokenized_text_phrases = [bigram[doc] for doc in tokenized_text]\n",
        "\n",
        "    # Train the Word2Vec model on the tokenized text with phrases\n",
        "    model_phrases = Word2Vec(tokenized_text_phrases, vector_size=100, window=5, min_count=5, workers=cores, sg=0)\n",
        "\n",
        "    # Save the trained models\n",
        "    model.save(f\"word2vec_{locale.lower()}.model\")\n",
        "    model_phrases.save(f\"word2vec_phrases_{locale.lower()}.model\")\n",
        "\n",
        "# Function to build and train the recommendation model\n",
        "def train_recommendation_model(locale):\n",
        "    # Load the training data for the specified locale\n",
        "    products_train = pd.read_csv(\"/content/drive/MyDrive/products_train_processed.csv\")\n",
        "    sessions_train = pd.read_csv(\"/content/drive/MyDrive/sessions_train.csv\")\n",
        "    sessions_train_locale = sessions_train[sessions_train['locale'] == locale]\n",
        "    merged_data = pd.merge(sessions_train_locale, products_train, left_on='next_item', right_on='id', how='left')\n",
        "    filtered_data = merged_data[['prev_items', 'next_item']]\n",
        "    filtered_data['prev_items'] = filtered_data['prev_items'].apply(lambda x: x.split())\n",
        "    input_features = filtered_data['prev_items']\n",
        "    target_labels = filtered_data['next_item']\n",
        "\n",
        "    # Load the Word2Vec model\n",
        "    model = Word2Vec.load(f\"word2vec_{locale.lower()}.model\")\n",
        "\n",
        "    # Build the recommendation model\n",
        "    vocab_size = len(model.wv.vocab)\n",
        "    embed_dim = 300\n",
        "    num_heads = 4\n",
        "    feed_forward_dim = 512\n",
        "\n",
        "    inputs = Input(shape=(None,))\n",
        "    embedding = Embedding(vocab_size, embed_dim)(inputs)\n",
        "    transformer = Transformer(num_heads=num_heads, d_model=embed_dim,\n",
        "                              ff_dim=feed_forward_dim, activation='relu')(embedding)\n",
        "    outputs = Dense(vocab_size, activation='softmax')(transformer)\n",
        "    recommendation_model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    optimizer = tf.keras.optimizers.Adam()\n",
        "    recommendation_model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "    # Convert input features and target labels to tensors\n",
        "    input_features = tf.ragged.constant(input_features.tolist())\n",
        "    target_labels = tf.convert_to_tensor(target_labels)\n",
        "\n",
        "    # Train the model\n",
        "    num_epochs = 10\n",
        "    recommendation_model.fit(input_features, target_labels, epochs=num_epochs)\n",
        "\n",
        "    # Return the trained model\n",
        "    return recommendation_model\n",
        "\n",
        "# Function to generate recommendations for a given locale and test data\n",
        "def generate_recommendations(locale, test_data):\n",
        "    # Load the Word2Vec and recommendation models\n",
        "    model = Word2Vec.load(f\"word2vec_{locale.lower()}.model\")\n",
        "    recommendation_model = train_recommendation_model(locale)\n",
        "\n",
        "    # Preprocess the test data for the specified locale\n",
        "    test_data_locale = test_data[test_data['locale'] == locale]\n",
        "    test_data_locale['prev_items'] = test_data_locale['prev_items'].apply(lambda x: x.split())\n",
        "    input_features = test_data_locale['prev_items'].tolist()\n",
        "    input_features = tf.ragged.constant(input_features)\n",
        "\n",
        "    # Generate predictions\n",
        "    predictions = recommendation_model.predict(input_features)\n",
        "\n",
        "    # Get the top 100 recommended product IDs for each session\n",
        "    recommended_product_ids = []\n",
        "    for prediction in predictions:\n",
        "        top_product_ids = tf.argsort(prediction, direction='DESCENDING')[:100]\n",
        "        recommended_product_ids.append(top_product_ids)\n",
        "\n",
        "    # Convert the recommended product IDs to a list\n",
        "    recommended_product_ids = [ids.numpy().tolist() for ids in recommended_product_ids]\n",
        "\n",
        "    # Return the recommended product IDs\n",
        "    return recommended_product_ids\n",
        "\n",
        "# Load the test data\n",
        "sessions_test = pd.read_csv(\"sessions_test_task1_phase1.csv\")\n",
        "\n",
        "# Generate recommendations for each locale and save the results\n",
        "locales = ['DE', 'JP', 'ES', 'FR', 'IT']\n",
        "for locale in locales:\n",
        "    # Train the Word2Vec model for the locale\n",
        "    train_word2vec_model(locale)\n",
        "\n",
        "    # Generate recommendations for the locale\n",
        "    recommended_product_ids = generate_recommendations(locale, sessions_test)\n",
        "\n",
        "    # Create a DataFrame with the results\n",
        "    results = pd.DataFrame({\n",
        "        'prev_items': sessions_test[sessions_test['locale'] == locale]['prev_items'],\n",
        "        'locale': sessions_test[sessions_test['locale'] == locale]['locale'],\n",
        "        'recommended_product_ids': recommended_product_ids\n",
        "    })\n",
        "\n",
        "    # Save the results to a CSV file\n",
        "    results.to_csv(f\"RESULTS_{locale}.csv\", index=False)\n"
      ]
    }
  ]
}